<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-06E12DG85Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-06E12DG85Q');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.ico"  type="image/x-icon">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title">SnapMem: Snapshot-based 3D Scene Memory for Embodied Exploration and Reasoning</h1> -->
          <h1 class="title is-1 publication-title">
            3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning
          </h1>          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yyuncong.github.io/">Yuncong Yang</a>*<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://hanyangclarence.github.io/">Han Yang</a>*<sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/jiachen-zhou5/">Jiachen Zhou</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://peihaochen.github.io/">Peihao Chen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://icefoxzhx.github.io/">Hongxin Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://yilundu.github.io/">Yilun Du</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UMass Amherst,</span>
            <span class="author-block"><sup>2</sup>CUHK,</span>
            <span class="author-block"><sup>3</sup>Columbia University,</span>
            <span class="author-block"><sup>4</sup>MIT</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(* indicates equal contribution)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://www.arxiv.org/abs/2411.17735"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=PbnWizEJL8w"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/UMass-Foundation-Model/3D-Mem"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-four-fifths">

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Video</h2> -->
        <div class="publication-video">
          <video src="static/videos/video_intro.mp4" controls autoplay muted
                style="width: 100%; height: auto;">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column" style="max-width: 60%; margin: 0 auto;">
        <h2 class="title is-3" style="padding-top: 80px;">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Constructing compact and informative 3D scene representations is essential for effective embodied exploration and reasoning, especially in complex environments over long periods. Existing scene representations, such as object-centric 3D scene graphs, have significant limitations. They oversimplify spatial relationships by modeling scenes as individual objects, with inter-object relationships described by restrictive texts, making it difficult to answer queries that require nuanced spatial understanding. Furthermore, these representations lack natural mechanisms for active exploration and memory management, which hampers their application to lifelong autonomy. In this work, we propose SnapMem, a novel snapshot-based scene representation serving as 3D scene memory for embodied agents. SnapMem employs informative images, termed Memory Snapshots, to capture rich visual information of explored regions. It also integrates frontier-based exploration by introducing Frontier Snapshots—glimpses of unexplored areas—that enable agents to make informed exploration decisions by considering both known and potential new information. Meanwhile, to support lifelong memory in active exploration settings, we further present an incremental construction pipeline for SnapMem, as well as an effective memory retrieval technique for memory management. Experimental results on three benchmarks demonstrate that SnapMem significantly enhances agents' exploration and reasoning capabilities in 3D environments over extended periods, highlighting its potential for advancing applications in embodied AI.           
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Abstract. -->

    
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Constructing compact and informative 3D scene representations is essential for effective embodied exploration and reasoning, especially in complex environments over extended periods. Existing representations, such as object-centric 3D scene graphs, oversimplify spatial relationships by modeling scenes as isolated objects with restrictive textual relationships, making it difficult to address queries requiring nuanced spatial understanding. Moreover, these representations lack natural mechanisms for active exploration and memory management, hindering their application to lifelong autonomy. In this work, we propose 3D-Mem, a novel 3D scene memory framework for embodied agents. 3D-Mem employs informative multi-view images, termed Memory Snapshots, to represent the scene and capture rich visual information of explored regions. It further integrates frontier-based exploration by introducing Frontier Snapshots—glimpses of unexplored areas—enabling agents to make informed decisions by considering both known and potential new information. To support lifelong memory in active exploration settings, we present an incremental construction pipeline for 3D-Mem, as well as a memory retrieval technique for memory management. Experimental results on three benchmarks demonstrate that 3D-Mem significantly enhances agents' exploration and reasoning capabilities in 3D environments, highlighting its potential for advancing applications in embodied AI.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Methodology. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <!-- <h3 class="title is-4">How to empower VLM agents with lifelong exploration and reasoning abilities?</h3> -->
        <h3 class="title is-4" style="font-size: 1.46rem;">How to unleash the power of VLM agents in lifelong exploration and reasoning?</h3>
        <h4 class="title is-5" style="padding-top: 20px; font-size: 1.2rem;">1. Incremental Construction of 3D Scene Memory</h4>

        <video style="transform: scale(1.2); width: 80%; padding-top: 30px; padding-bottom: 40px;" autoplay muted loop playsinline preload="auto">
          <source src="static/videos/demo_gif_1.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>

        <div class="content has-text-justified">
          <!-- <p>
            At each timestep, given a stream of egocentric views, <strong>SnapMem</strong> performs the following steps:
          </p> -->
          <ol style="text-align: left;">
            <li>Detect object instances and construct object nodes.</li>
            <li>Adds newly detected objects.</li>
            <li>Updates 3D scene memory using the Co-Visibility Clustering algorithm.</li>
          </ol>
        </div>
        <h4 class="title is-5" style="padding-top: 30px; font-size: 1.2rem;">2. Retrieval and Reasoning of 3D Scene Memory</h4>

        <video style="width: 80%; padding-top: 0px; padding-bottom: 10px;" autoplay muted loop playsinline preload="auto">
          <source src="static/videos/demo_gif_2.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>

        <div class="content has-text-justified">
          <p>
            The 3D scene memory is efficiently filtered using <strong>Prefiltering</strong>. The VLM agent then leverages:
          </p>
          <ul style="text-align: left;">
            <li><em>Memory Snapshots</em>—filtered scene memory for explored regions.</li>
            <li><em>Frontier Snapshots</em>—glimpses of unexplored regions.</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Methodology. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Demo -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Demos</h2>

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <div class="content has-text-justified">
          <!-- <p> 
            We present demos of <strong>Embodied Q&A</strong> in <strong>lifelong</strong> exploration scenarios. 
            The agent is initialized at a specific location in an unknown environment and tasked with <strong>answering a series of questions</strong>. 
            The agent either selects a frontier to explore or uses the scene memory to reason about the questions. 
          </p>  -->
          <p> 
            Each demo represents an Embodied Q&A episode in a Habitat-sim scene, where the agent is required to answer 6-8 questions in sequence. 
            <!-- In each frame, the current question is displayed at the top, followed by four columns of illustrations:  -->
          </p>
          <p>
            <strong>Top-down map.</strong> The notations are explained in the following:
              <!-- Light green areas indicate explored regions, light grey areas represent unexplored regions, and black areas denote obstacles. 
              Each purple arrow marks a frontier with its direction. A circular sector and the enclosed dots of the same color represent a memory snapshot, 
              where each dot corresponds to an object in that snapshot, and the center of the sector is the observation position of the memory snapshot. 
              The red star shows the target location the agent is currently navigating towards.  -->

              <div class="legend">
                <img src="static/images/legend.png" alt="Legend for Top-down Map" />
              </div>
          </p>
          <!-- <ul> 
            <li> 
              <strong>Top-down map.</strong> The notations in the top-down map are explained in the following:
              <div class="legend">
                <img src="static/images/legend.png" alt="Legend for Top-down Map" />
              </div>

            </li> 
            <li> 
              <strong>Egocentric views.</strong> 
            </li> 
            <li> 
              <strong>The agent's decision</strong>. The reason for the choice is displayed at the bottom of the frame.
            </li> 
            <li>
              <strong>All relevant snapshots.</strong> All the current frontier snapshots at the top and all the filtered memory snapshot at the bottom. 
              The highlighted snapshot is the choice at the current step.
            </li> 
          </ul> -->
        </div>

        <!-- <div class="columns is-vcentered interpolation-panel" style="margin-top: 10mm;">
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper-1">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider-1"
                   step="1" min="0" max="107" value="0" type="range">
          </div>
        </div>
        <br/>

        <div class="columns is-vcentered interpolation-panel" style="margin-top: 10mm;">
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper-2">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider-2"
                   step="1" min="0" max="106" value="0" type="range">
          </div>
        </div>
        <br/>

        <div class="columns is-vcentered interpolation-panel" style="margin-top: 10mm;">
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper-3">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider-3"
                   step="1" min="0" max="91" value="0" type="range">
          </div>
        </div>
        <br/>

        <div class="columns is-vcentered interpolation-panel" style="margin-top: 10mm;">
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper-4">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider-4"
                   step="1" min="0" max="118" value="0" type="range">
          </div>
        </div>
        <br/> -->

        <div class="columns is-centered has-text-centered" style="margin-bottom: -15mm; margin-top: 30mm; ">
          <div class="column">
            <div class="publication-video">
              <video src="static/videos/qualitative_1.mp4" controls autoplay muted
                    style="width: 100%; height: auto;">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>

        <div class="columns is-centered has-text-centered" style="margin-bottom: -15mm;">
          <div class="column">
            <div class="publication-video">
              <video src="static/videos/qualitative_2.mp4" controls autoplay muted
                    style="width: 100%; height: auto;">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>

        <div class="columns is-centered has-text-centered" style="margin-bottom: -15mm;">
          <div class="column">
            <div class="publication-video">
              <video src="static/videos/qualitative_3.mp4" controls autoplay muted
                    style="width: 100%; height: auto;">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column">
            <div class="publication-video">
              <video src="static/videos/qualitative_4.mp4" controls autoplay muted
                    style="width: 100%; height: auto;">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>



        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            Our work is built on 
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
